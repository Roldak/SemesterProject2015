\section{Redesigning MbArrays}
\label{sec:redesign}

After having exposed the flow in the current |MbArray| design that slows down its performance, I present the approach I took in order to address it.

\subsection{The Goal}

The goal is to keep the mechanisms in |MbArray|s that are working, but to decrease the memory needed for that.
To this end, the design change I opted for was adding more specializations to the |MbArray|s: one for each primitive type that exists in Scala. In other words, an |MbArray[Boolean]| would get transformed to |MbArray_B| which uses a |boolean[]| internally instead of the prevous |long[]|, and so on equivalently for every of the 10 primitive types that exist in Scala. Hopefully, this would result in less heap memory wasted and therefore a higher GC throughput.

\input{BenchmarkOutputs/NewBenchSM.tex}

\subsection{Design transformation}

Adding the 7 other specializations was fairly straightforward: All that had to be done was duplicating one of the two existing specialization for every new variants and changing the internal array type |Array[Long]| to the ones adapted for the different specialization.
Then, the code that handled |MbArray| instantiations, namely |MbArray_empty_J|, |MbArray_empty_D|, |MbArray_clone_J| and |MbArray_clone_D|, had to be modified in order to take into accounts the new specializations.
For example:

\begin{lstlisting-nobreak-java}
public static <T> MbArray<T> mbArray_empty_J(int size, byte T_Tag) {
  return new MbArray_J<T>(T_Tag, size);
}
\end{lstlisting-nobreak-java}

Was transformed to:

\begin{lstlisting-nobreak-java}
public static <T> MbArray<T> mbArray_empty_J(int size, byte T_Tag) {
  switch(T_Tag) {
  case MiniboxConstants.LONG:
    return new MbArray_J<T>(size);
  case MiniboxConstants.INT:
    return new MbArray_I<T>(size);
  case MiniboxConstants.SHORT:
    return new MbArray_S<T>(size);
  case MiniboxConstants.CHAR:
    return new MbArray_C<T>(size);
  case MiniboxConstants.BYTE:
    return new MbArray_B<T>(size);
  case MiniboxConstants.BOOLEAN:
    return new MbArray_Z<T>(size);
  case MiniboxConstants.UNIT:
    return new MbArray_V<T>(size);
  default:
    return new MbArray_L<T>(size);
  }
}
\end{lstlisting-nobreak-java}

Similarly, the |apply| and |update| methods had to be changed.
For example:

\begin{lstlisting-nobreak-java}
public static <T> long mbArray_apply_J(MbArray<T> mbArray, int index, byte T_Tag) {
  if (mbArray instanceof MbArray_J<?>)
    return ((MbArray_J<?>)mbArray).apply_J(index);
  else
    return MiniboxConversionsLong.box2minibox_tt(
    	mbArray.apply(index), T_Tag);
}
\end{lstlisting-nobreak-java}

Became:

\begin{lstlisting-nobreak-java}
public static <T> long mbArray_apply_J(MbArray<T> mbArray, int index, byte T_Tag) {
  if (mbArray instanceof MbArray_J<?>)
    return ((MbArray_J<?>)mbArray).apply_J(index);
  else if (mbArray instanceof MbArray_I<?>)
    return ((MbArray_I<?>)mbArray).apply_J(index);
  else if (mbArray instanceof MbArray_S<?>)
    return ((MbArray_S<?>)mbArray).apply_J(index);
  else if (mbArray instanceof MbArray_C<?>)
    return ((MbArray_C<?>)mbArray).apply_J(index);
  else if (mbArray instanceof MbArray_B<?>)
    return ((MbArray_B<?>)mbArray).apply_J(index);
  else if (mbArray instanceof MbArray_Z<?>)
    return ((MbArray_Z<?>)mbArray).apply_J(index);
  else if (mbArray instanceof MbArray_V<?>)
    return ((MbArray_V<?>)mbArray).apply_J(index);
  else
    return MiniboxConversionsLong.<T>box2minibox_tt(
    	mbArray.apply(index), T_Tag);
}
\end{lstlisting-nobreak-java}

\subsection{Re-benchmarking}

Following the transformation, we re-benchmarked out example programs using the new |MbArray|.
 
\textbf{Initial benchmark.} Table \ref{table:NewCTvsMB} shows that the new version performs approximately at the same level as the |ClassTag| version on this benchmark. Moreover, figure \ref{fig:NewGcComp} shows how the new version spends 3$\times$ less time collecting garbage than the old version, from which follows a 100ms runtime performance gain. One can note that one top of the gain coming from the reduced amount of time spent collecting garbage, the new version gets a boost of 70ms from the fact that less memory is used, thus reducing the amount of memory that needs to be read. In average, the new version is 25\% faster than the old version in this benchmark.

\begin{figure}
\subfigure[ClassTag version]{
	\lstinputlisting{BenchmarkOutputs/CtVector1.txt}
}
\subfigure[Old Miniboxed version]{
	\lstinputlisting{BenchmarkOutputs/MbVector1.txt}
}
\subfigure[New Miniboxed version]{
	\lstinputlisting{BenchmarkOutputs/NewMbVector1.txt}
}
\caption{New benchmark outputs for ArrayBuffers of 10'000'000 elements}
\label{fig:NewGcComp}
\end{figure}

As one can notice, even the new |MbArray| version does not seem to be a lot faster than the |ClassTag| version on that initial benchmark. 

\textbf{Mapping to Floats.} However, most of the benchmark \emph{do} show better numbers for the new |MbArray| version. For example, another benchmark was developped, which shows that the version using new |MbArray|s can be approximately 250\% faster than the version using |ClassTag|s and 20\% faster than the version using the old |MbArray|s. This can be seen in table \ref{table:OtherCTvsMB}. Note that the only difference in the source code between this benchmark and the initial one is that instead of:

\begin{lstlisting-nobreak}
  using(bufs) setUp {
    b => 
      b.map(_ + 1)
      b.map(_ + 2)
  } in {
    b => b.map(_ + 1)
  }
\end{lstlisting-nobreak}

It is doing : 

\begin{lstlisting-nobreak}
  using(bufs) setUp {
    b => 
      b.map(_ + 1)
      b.map(_ + 2)
  } in {
    b => b.map(_ + 1).map(_ + 2.5f).map(_ + 3)
  }
\end{lstlisting-nobreak}

Also, figure \ref{fig:OtherGcComp}, which corresponds to a handmade version of this new benchmark, shows how the version using new |MbArray|s spends approximately $650\%$ less time collecting garbage.

\input{BenchmarkOutputs/OtherBenchSM.tex}

\begin{figure}
\subfigure[ClassTag version]{
	\lstinputlisting{BenchmarkOutputs/CtVector3.txt}
}
\subfigure[Old Miniboxed version]{
	\lstinputlisting{BenchmarkOutputs/MbVector3.txt}
}
\subfigure[New Miniboxed version]{
	\lstinputlisting{BenchmarkOutputs/NewMbVector3.txt}
}
\caption{New benchmark outputs for ArrayBuffers of 10'000'000 elements with mappings to Floats}
\label{fig:OtherGcComp}
\end{figure}

\textbf{Mapping to Longs and Doubles.} There is another case which needs to be benchmarked, corresponding to the best case scenario for old |MbArray|s: mapping to |Long|s and |Double|s. The reason it is the best scenario is that it is the one in which no extra space is wasted by the old |MbArray| implementation. Here is the benchmark code:

\begin{lstlisting-nobreak}
  using(bufs) setUp {
    b => 
      b.map(_ + 1L)
      b.map(_ + 2L)
  } in {
    b => b.map(_ + 1L).map(_ + 2.5).map(_ + 1.5)
  }
\end{lstlisting-nobreak}

Results for this benchmark can be seen in table \ref{table:WorstCTvsMB}, which show that we did not lose any run time performance by changing the design.

\subsection{JVM Memory}

It is also worth noting that the overall speed difference increases at the advantage of the new |MbArray|s when less memory is assigned to the JVM. For example, running the benchmark on a JVM which has less than 400MB completely crashes the version using the old |MbArray|s by throwing an |OutOfMemory| exception. When the amount of memory assigned is between 400MB and 512MB, the old |MbArray| version runs but performs really bad compared to the two other versions, spending a considerable amount of time in the GC. Finally, when more than 512MB is assigned, we obtain the numbers that have been shown so far (Tables \ref{table:NewCTvsMB} and \ref{table:OtherCTvsMB} as well as figures \ref{fig:NewGcComp} and \ref{fig:OtherGcComp}), since each of these benchmarks have been ran on a JVM to which was assigned at least 1GB.

\subsection{Conclusion}

In every cases, the version using new |MbArray|s is either faster (tables \ref{table:NewCTvsMB} or \ref{table:OtherCTvsMB}) than or as fast (table \ref{table:WorstCTvsMB}) as the version using old |MbArray|s. Futhermore, although it is in some cases running as equivalently fast as the |ClassTag| version -- as shows benchmark \ref{table:NewCTvsMB} --, it is actually in most cases faster than the |ClassTag| version, tables \ref{table:OtherCTvsMB} and \ref{table:WorstCTvsMB} being a good example of this. 

\input{BenchmarkOutputs/WorstBenchSM.tex}